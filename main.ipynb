{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset from DATA.GOV\n",
    "Link : https://catalog.data.gov/dataset/air-quality/resource/f3ed1638-92da-4f88-bb6b-7d3940514574"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1 Explore Data Analysis (EDA)\n",
    " Load the dataset and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://data.cityofnewyork.us/api/views/c3uy-2p5r/rows.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display dataset structure\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://data.cityofnewyork.us/api/views/c3uy-2p5r/rows.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Display dataset structure\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Data Cleaning and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Handling missing values: Drop rows with missing 'Geo Join ID' or 'Data Value'\n",
    "data = data.dropna(subset=['Geo Join ID', 'Data Value'])\n",
    "\n",
    "# b. Converting 'Start_Date' to datetime format\n",
    "data['Start_Date'] = pd.to_datetime(data['Start_Date'], format='%m/%d/%Y')\n",
    "\n",
    "# c. Remove irrelevant columns\n",
    "# Drop the 'Message' and 'Indicator ID' columns as these don't contribute to the model\n",
    "data = data.drop(columns=['Message', 'Indicator ID'])\n",
    "\n",
    "# X: Select all columns except 'Data Value' (target)\n",
    "X = data.drop(columns=['Data Value'])  \n",
    "y = data['Data Value']\n",
    "\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Data Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Distribution of the target variable (Data Value)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['Data Value'], bins=30, kde=True)\n",
    "plt.title('Distribution of Data Value')\n",
    "plt.xlabel('Data Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# b. Time series analysis of Data Value over Start_Date\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='Start_Date', y='Data Value', data=data)\n",
    "plt.title('Data Value Over Time')\n",
    "plt.xlabel('Start Date')\n",
    "plt.ylabel('Data Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# c. Bar plot to compare Data Value across different Geo Place Names\n",
    "plt.figure(figsize=(12, 6))\n",
    "geo_means = data.groupby('Geo Place Name')['Data Value'].mean().sort_values(ascending=False).head(10)\n",
    "sns.barplot(x=geo_means.values, y=geo_means.index)\n",
    "plt.title('Top 10 Geo Place Names by Average Data Value')\n",
    "plt.xlabel('Average Data Value')\n",
    "plt.ylabel('Geo Place Name')\n",
    "plt.show()\n",
    "\n",
    "# d. Scatter plot for Geo Join ID vs Data Value\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Geo Join ID', y='Data Value', data=data)\n",
    "plt.title('Geo Join ID vs Data Value')\n",
    "plt.xlabel('Geo Join ID')\n",
    "plt.ylabel('Data Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4 Model Development\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree Regressor': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest Regressor': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_preprocessed, y_train)\n",
    "    y_pred = model.predict(X_test_preprocessed)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f'{name} R-squared: {r2:.4f}')\n",
    "\n",
    "# Hyperparameter Tuning for Gradient Boosting Regressor\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=models['Gradient Boosting Regressor'], param_grid=param_grid, scoring='r2', cv=5)\n",
    "grid_search.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "y_pred_best_gb = best_gb_model.predict(X_test_preprocessed)\n",
    "r2_best_gb = r2_score(y_test, y_pred_best_gb)\n",
    "print(f'Best Gradient Boosting Regressor R-squared: {r2_best_gb:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all R-squared scores\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_preprocessed, y_train)\n",
    "    y_pred = model.predict(X_test_preprocessed)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f'{name} R-squared: {r2:.4f}')\n",
    "\n",
    "print(f'Best Gradient Boosting Regressor R-squared: {r2_best_gb:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Engineering\n",
    "We began by cleaning the dataset, handling missing values, and converting the 'Start_Date' column to a datetime format. We then dropped irrelevant columns like 'Message' and 'Indicator ID'. The features were split into numerical and categorical types, and we applied preprocessing steps such as standard scaling for numerical features and one-hot encoding for categorical features.\n",
    "\n",
    "## Model Selection and Training\n",
    "We explored multiple regression models, including Linear Regression, Decision Tree Regressor, Random Forest Regressor, and Gradient Boosting Regressor. Each model was trained on the preprocessed data.\n",
    "\n",
    "## Model Evaluation and Tuning\n",
    "The initial evaluation showed that the Gradient Boosting Regressor had the highest R² score among the models. To further improve its performance, we employed GridSearchCV for hyperparameter tuning, which helped us find the best configuration for the Gradient Boosting Regressor. This process allowed us to fine-tune the model and enhance its predictive capabilities.\n",
    "\n",
    "## Results and Insights\n",
    "The Gradient Boosting Regressor, after hyperparameter tuning, achieved an R² score that was the highest among all the models tested. This suggests that the Gradient Boosting Regressor was better at capturing the underlying patterns in the data compared to the other models.\n",
    "\n",
    "## Conclusion\n",
    "In conclusion, our analysis demonstrated the importance of thorough data preprocessing, feature engineering, and model selection in building a predictive model. The Gradient Boosting Regressor, with its superior performance after hyperparameter tuning, proved to be the most effective model for this dataset. This outcome highlights the effectiveness of ensemble methods in capturing complex interactions and non-linear relationships within the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
